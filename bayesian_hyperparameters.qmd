---
format: 
  html:
    code-fold: true
jupyter: python3
highlight-style: github
---

## Motivation

I recently became interested in Bayesian optimization as a powerful strategy for hyperparameter tuning, especially in scenarios where there are several hyperparameters that interact in complex ways. This makes it particularly appealing for models like XGBoost, LightGBM, and CatBoost as they rely heavily on careful tuning for optimal performance.

I'm particularly interested in using the `optuna` library due to its flexibility, support for early stopping, and rich feature set. While I’m aware of alternatives like `scikit-optimize`’s `BayesSearchCV`, `optuna` offers greater customization and more intuitive control. Additionally, I believe that skills developed with `optuna` will transfer more easily to other frameworks than the reverse.




## The HIGGS dataset

The HIGGS dataset is a binary classification dataset originally developed for high-energy physics research and later adopted in machine learning competitions. It contains 28 continuous features derived from simulated particle collisions, and the target variable indicates whether an event corresponds to a Higgs Boson decay.

I selected this dataset because of its large size, which makes it ideal for testing hyperparameter optimization strategies on computationally intensive models. While I don’t fully understand the meaning of each variable, that’s not the goal of this analysis. The dataset serves well as a realistic, high-volume benchmark for tuning performance.

Here's a snapshot of the dataset:

```{python data_libraries}
#| echo: false

import os, gc
import polars as pl
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
from jinja2 import Template
from IPython.display import HTML, display

FORCE_RERUN = False
```

```{python load_data}
#| echo: false


if not os.path.exists('./higgs_data/HIGGS.parquet') or FORCE_RERUN:
    from download_data import main as download_data
    download_data()
    
df = pl.scan_parquet('./higgs_data/HIGGS.parquet')
df.head(10).collect().to_pandas().to_html()
```

### Higgs Boson Variable

Below we can observe the distribution of the target variable. It's almost equally balanced, with 52% of the events corresponding to a Higgs Boson decay and 48% of the events not corresponding to a Higgs Boson decay.

```{python}
#| echo: false
#| out-width: "70%"
#| fig-align: center


plotting_df = (
    df.with_columns([
        pl.when(pl.col("target") == 0).then(pl.lit("No"))
         .when(pl.col("target") == 1).then(pl.lit("Yes"))
         .otherwise(pl.lit("Unknown"))
         .alias("Higgs Boson Decay")
    ])
    .collect()
)

counts = plotting_df.group_by('Higgs Boson Decay').len()

palette = {"Yes": "#4caf50", "No": "#f44336"}
plt.figure(figsize=(8, 6))

ax = sns.barplot(x="Higgs Boson Decay", y="len", hue="Higgs Boson Decay", 
                 data=counts, legend=False, palette=palette)

# Add count labels on top of bars
for bar in ax.patches:
    count = int(bar.get_height())
    ax.annotate(
        f"{count:,}",  # comma formatting
        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
        xytext=(0, 5),
        textcoords="offset points",
        ha="center", va="bottom",
        fontsize=12, fontweight="bold"
    )


plt.title("Counts of Higgs Boson Decay", fontsize=16)
plt.xlabel("Higgs Boson Decay", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.ticklabel_format(style='plain', axis='y')  
plt.grid(True, axis="y", linestyle="--", alpha=0.3)
plt.tight_layout()
plt.show()
```

### Higgs Boson Variable vs Predictor Variables

Below is a dropdown menu that lets you explore the distribution of each predictor variable, separated by whether or not a Higgs boson decay occurred. For efficiency, these plots are based on a subsample of 50,000 observations.

At first glance, there are no major differences in the distributions of the predictors between the two outcome classes. This suggests that any signal present is likely subtle and embedded in complex, multidimensional interactions. Gradient Boosting Machines are a strong choice for this task.

```{python, boxplot_violin}
#| echo: false

def boxplot_violin(
    data, x, y,
    title="Half Violin + Boxplot",
    palette="Set2",
    bw_adjust=0.3,
    violin_side="right",  # or "left"
    ax=None
):
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 6))

    categories = data[x].unique()

    if type(palette) == str:
        pal = sns.color_palette(palette, len(categories))
    elif type(palette) == list or type(palette) == dict:
        pal = palette
    else:
        raise ValueError("palette must be a string or a list or a dict")

    # Violin plot
    sns.violinplot(
        data=data,
        x=x, y=y,
        hue=x,
        palette=pal,
        inner=None,
        cut=0,
        bw_adjust=bw_adjust,
        linewidth=0,
        legend=False,
        ax=ax,
    )

    # Clip to half and add outlines
    for artist in ax.collections:
        path = artist.get_paths()[0]
        vertices = path.vertices
        mean_x = np.mean(vertices[:, 0])
        if violin_side == "right":
            vertices[:, 0] = np.where(vertices[:, 0] < mean_x, mean_x, vertices[:, 0])
        elif violin_side == "left":
            vertices[:, 0] = np.where(vertices[:, 0] > mean_x, mean_x, vertices[:, 0])
        else:
            raise ValueError("violin_side must be 'right' or 'left'")
        artist.set_edgecolor("black")
        artist.set_linewidth(0.5)

    # Boxplot — same trick
    sns.boxplot(
        data=data,
        x=x, y=y,
        hue=x,
        palette=pal,
        width=0.05,
        boxprops={ 'edgecolor': 'black', 'linewidth': 1.2 },
        capprops={ 'color': 'black', 'linewidth': 1 },
        whiskerprops={ 'color': 'black', 'linewidth': 1 },
        flierprops={ 'markeredgecolor': 'black', 'markersize': 3 },
        medianprops={ 'color': 'black', 'linewidth': 1 },
        legend=False,
        ax=ax,
    )

    ax.set_title(title)
    ax.legend_.remove() if ax.get_legend() else None
    return ax
```
```{python}
#| echo: false

plotting_df = plotting_df.sample(n=50_000, with_replacement=False).to_pandas()

# List of numeric features 
features = [col for col in plotting_df.columns if col not in ("target", "Higgs Boson Decay")]


for feature in features:
    fig, ax = plt.subplots(figsize=(6, 4))
    title = f'Distribution of {feature}\nby Higgs Boson Decay'
    boxplot_violin(
        x="Higgs Boson Decay", y=feature, 
        data=plotting_df, 
        ax=ax, 
        title=title,
        palette=palette,
    )
    plt.tight_layout()
    plt.savefig(f"./plots/boxplotviolins/{feature}.png", dpi=300)
    plt.close(fig)
```

```{python}
#| results: asis
#| echo: false


template = Template("""
<div class="mb-3" style="max-width: 300px;">
  <label for="feature-select" class="form-label">Select a feature:</label>
  <select id="feature-select" class="form-select">
    {% for f in features %}
      <option value="{{ f }}">{{ f }}</option>
    {% endfor %}
  </select>
</div>

<div id="plot-container">
  {% for f in features %}
  <img src="plots/boxplotviolins/{{ f }}.png"
       class="plot"
       id="plot-{{ f }}"
       style="display: {% if loop.first %}block{% else %}none{% endif %}; margin: 0 auto; width: 70%;" />
  {% endfor %}
</div>

<script>
  const select = document.getElementById("feature-select");
  const plots = document.querySelectorAll(".plot");

  select.addEventListener("change", () => {
    plots.forEach(p => p.style.display = "none");
    const selected = document.getElementById(`plot-${select.value}`);
    if (selected) selected.style.display = "block";
  });
</script>
""")

display(HTML(template.render(features=features)))
```

## Methodology

To make the most of Bayesian optimization, I structured the process into three stages. Across all stages, I applied a few consistent strategies:

- Subsampling the dataset for faster iteration and experimentation.
- Using a single train/validation split rather than cross-validation. While cross-validation is more robust for model selection, this project is focused on exploring Bayesian optimization techniques rather than identifying a definitive best model. For future, more rigorous applications, I plan to incorporate cross-validation.
- Employing pruners to stop trials early when no further improvement is observed. This improves efficiency and allows more trials to be completed within the same time budget.  
  - Note: CatBoost's API currently does not work well with Optuna's pruning mechanisms, so pruning was disabled for CatBoost experiments.

### Stages

1. Coarse, Broad Search  
   - Subsample 10% of the data.  
   - Run 100 Optuna trials to identify which hyperparameters are most impactful.  
   - Use 100 boosting rounds for fast feedback.  
   - Use a `MedianPruner` for gentle early stopping.  
   - Purpose: Narrow the search space by identifying influential parameters.

2. Fine, Narrow Search  
   - Subsample 20% of the data.  
   - Focus on tuning the most important hyperparameters identified in Stage 1, along with key regularization parameters.  
   - Run 50 Optuna trials with a more focused search space.  
   - Increase to 1000 boosting rounds for more thorough learning.  
   - Use a `SuccessiveHalvingPruner` to speed up the search while still exploring promising trials.

3. Fit Final Model  
   - Use the full dataset.  
   - Train a final model using the best hyperparameters found in Stage 2.  
   - Evaluate final performance on a held-out test set.
   - Compare against an untuned model to assess the impact of hyperparameter tuning.

## Tuning The Models


```{python}
#| echo: false

import numpy as np, pandas as pd 
import lightgbm as lgb, xgboost as xgb, catboost as cat
from sklearn.metrics import roc_auc_score

import optuna, pickle
import optuna.visualization as vis
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner, SuccessiveHalvingPruner
from optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback
optuna.logging.set_verbosity(optuna.logging.WARNING)

from manual_splits import polars_train_test_split, file_train_test_split, file_subsample, csv_no_headers
from contextlib import redirect_stdout, redirect_stderr
```


```{python}
#| echo: false

# Prepare the dataset for training

# Cast the target column to integer type
df = df.with_columns(
    pl.col("target").cast(pl.Int32)
).collect()

# Split the data into training and testing sets
train_df, test_df = polars_train_test_split(df, random_state=42)

# Save the data to parquet files for later / efficient retrieval
train_df.write_parquet('./higgs_data/train.parquet')
test_df.write_parquet('./higgs_data/test.parquet')

# Column Description File for Catboost
with open('./higgs_data/train.cd', 'w') as f:
    type_selector = {
        pl.Int32: "Num",
        pl.Int64: "Num",
        pl.Float32: "Num",
        pl.Float64: "Num",
        pl.Boolean: "Categ",
        pl.Categorical: "Categ"
    }
    
    lines = [f'{i}\t{type_selector[type_]}\n' for i, type_ in enumerate(df.dtypes)]
    lines[0] = '0\tLabel\n'

    f.writelines(lines)

# Create necessary directories
os.makedirs('./studies', exist_ok=True)
os.makedirs('./models', exist_ok=True)

# Files already saved - Free up memory 
del df, train_df, test_df
_ = gc.collect()    
```

```{python}
#| echo: false

def print_study(study):
    # 1. Best trial summary
    best = study.best_trial
    print(f"Best AUC: {best.value:.6f}")
    print("Best Parameters:")
    for k, v in best.params.items():
        print(f"  {k}: {v}")

    # 2. Trial counts
    completed = sum(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials)
    pruned = sum(t.state == optuna.trial.TrialState.PRUNED for t in study.trials)

    print(f"\nTrials Completed:\t{completed}")
    print(f"Trials Pruned:\t{pruned}")
    print(f"Total Trials:\t{len(study.trials)}")

    # 3. Optimization history 
    vis.plot_optimization_history(study).show()

    # 4. Parameter importances 
    vis.plot_param_importances(study).show()

    # 5. Slice plot
    vis.plot_slice(study).show()
```

```{python}
#| echo: false

def new_range(trial, previous_study, param, 
              margin=0.2, lower_bound=None, upper_bound=None):

    value = previous_study.best_params[param]
    low = value * (1 - margin) 
    high = value * (1 + margin)

    if lower_bound is not None:
        low = max(low, lower_bound)
    if upper_bound is not None:
        high = min(high, upper_bound)

    # Return suggested range
    if isinstance(value, int):
        # high + 1 to make range inclusive when casting
        return trial.suggest_int(param, int(low), int(high) + 1)

    elif isinstance(value, float):
        return trial.suggest_float(param, low, high)

    else:
        raise ValueError(f"Unsupported type for parameter {param} of type {type(value)}")

```

::: {.panel-tabset}

### XGBoost

#### Stage 1

```{python}
#| code-summary: "Initial Tuning Trial for XGBoost models"

def xgb_objective_stage1(trial):
    params = {
        "tree_method": "hist", 
        "device": "cuda",
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "eta": trial.suggest_float("eta", 1e-3, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "lambda": trial.suggest_float("lambda", 0, 5.0),
        "alpha": trial.suggest_float("alpha", 0, 5.0),
        "min_child_weight": trial.suggest_float("min_child_weight", 0.1, 10.0, log=True),
    }

    split_folder = 'train/xgboost/stage1/data'
    nested_folder = 'train/xgboost/stage1/data/training'
    
    subsample = file_subsample(f'higgs_data/train.parquet', split_folder, sample_fraction=0.1)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)
    
    # Ensure no headers in the csv files for XGBoost
    train_set = csv_no_headers(train_set)
    valid_set = csv_no_headers(valid_set)
    holdout_set = csv_no_headers(holdout_set)


    dtrain = xgb.DMatrix(f"{train_set}?format=csv&label_column=0")
    dvalid = xgb.DMatrix(f"{valid_set}?format=csv&label_column=0")

    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = xgb.train(
                params,
                dtrain,
                num_boost_round=100,
                evals=[(dvalid, "validation")],
                early_stopping_rounds=15,
                callbacks=[XGBoostPruningCallback(trial, "validation-auc")]
            )


    dtest = xgb.DMatrix(f"{holdout_set}?format=csv&label_column=0")
    preds = model.predict(dtest)

    return roc_auc_score(dtest.get_label(), preds)
```

```{python}
#| echo: false

if os.path.exists("studies/xgb_stage1.pkl") and not FORCE_RERUN:
    with open("studies/xgb_stage1.pkl", "rb") as f:
        xgb_study1 = pickle.load(f)

else:
    xgb_study1 = optuna.create_study(
        study_name="xgb_stage1", 
        direction="maximize",
        sampler=TPESampler(n_startup_trials=20),
        pruner=MedianPruner(n_warmup_steps=20, interval_steps=5),
    )

    xgb_study1.optimize(xgb_objective_stage1, n_trials=100, gc_after_trial=True)

    with open("studies/xgb_stage1.pkl", "wb") as f:
        pickle.dump(xgb_study1, f)

print_study(xgb_study1)
```

#### Stage 2

```{python}
#| code-summary: "Second, Narrower Tuning Trial for XGBoost models"

def xgb_objective_stage2(trial):
    params = {
        "tree_method": "hist", 
        # "device": "cuda", # my system doesn't have enough VRAM
        "objective": "binary:logistic",
        "eval_metric": "auc",
        
        # Untouched as its related to the boosting rounds
        "eta": trial.suggest_float("eta", 1e-3, 0.3, log=True),

        # Hyperparameters to tune further
        "max_depth": new_range(trial, xgb_study1, "max_depth"),
        "subsample": new_range(trial, xgb_study1, "subsample", upper_bound=1.0),
        "colsample_bytree": new_range(trial, xgb_study1, "colsample_bytree", upper_bound=1.0),
        "lambda": new_range(trial, xgb_study1, "lambda"),
        "alpha": new_range(trial, xgb_study1, "alpha"),
        "min_child_weight": new_range(trial, xgb_study1, "min_child_weight"),
    }

    split_folder = 'train/xgboost/stage2/data'
    nested_folder = 'train/xgboost/stage2/data/training'
    
    subsample = file_subsample(f'higgs_data/train.parquet', split_folder, sample_fraction=0.2)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)
    
    # Ensure no headers in the csv files for XGBoost
    train_set = csv_no_headers(train_set)
    valid_set = csv_no_headers(valid_set)
    holdout_set = csv_no_headers(holdout_set)

    dtrain = xgb.DMatrix(f"{train_set}?format=csv&label_column=0")
    dvalid = xgb.DMatrix(f"{valid_set}?format=csv&label_column=0")

    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = xgb.train(
                params,
                dtrain,
                num_boost_round=1000,
                evals=[(dvalid, "validation")],
                early_stopping_rounds=50,
                callbacks=[XGBoostPruningCallback(trial, "validation-auc")]
            )


    dtest = xgb.DMatrix(f"{holdout_set}?format=csv&label_column=0")
    preds = model.predict(dtest)
    
    return roc_auc_score(dtest.get_label(), preds)
```

```{python}
#| echo: false

if os.path.exists("studies/xgb_stage2.pkl") and not FORCE_RERUN:
    with open("studies/xgb_stage2.pkl", "rb") as f:
        xgb_study2 = pickle.load(f)

else:
    xgb_study2 = optuna.create_study(
        study_name="xgb_stage2", 
        direction="maximize",
        pruner = SuccessiveHalvingPruner(min_resource=100, reduction_factor=3),
    )

    xgb_study2.optimize(xgb_objective_stage2, n_trials=50, gc_after_trial=True)

    with open("studies/xgb_stage2.pkl", "wb") as f:
        pickle.dump(xgb_study2, f)

print_study(xgb_study2)
```


#### Stage 3

```{python}
#| code-summary: "Final fitting for XGBoost model"

def xgb_objective_stage3(best_params):
    params = {
        "tree_method": "hist", 
        # "device": "cuda", # my system doesn't have enough VRAM
        "objective": "binary:logistic",
        "eval_metric": "auc",
    }

    split_folder = 'train/xgboost/stage3/data'

    train_set, valid_set  = file_train_test_split(f'higgs_data/train.parquet', split_folder, test_size=0.2)

    # Ensure no headers in the csv files for XGBoost
    train_set = csv_no_headers(train_set)
    valid_set = csv_no_headers(valid_set)

    dtrain = xgb.DMatrix(f"{train_set}?format=csv&label_column=0")
    dvalid = xgb.DMatrix(f"{valid_set}?format=csv&label_column=0")

    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            baseline_model = xgb.train(
                params,
                dtrain,
                num_boost_round=1000,
                evals=[(dvalid, "validation")],
                early_stopping_rounds=50,
            )

    baseline_model.save_model("models/baseline_xgb.json")

    params.update(best_params)
    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            tuned_model = xgb.train(
                params,
                dtrain,
                num_boost_round=1000,
                evals=[(dvalid, "validation")],
                early_stopping_rounds=50,
            )

    tuned_model.save_model("models/tuned_xgb.json")
```

```{python}
#| echo: false


conditions = [
    not os.path.exists("models/baseline_xgb.json"), 
    not os.path.exists("models/tuned_xgb.json"),
    FORCE_RERUN
]

if any(conditions):
    xgb_objective_stage3(xgb_study2.best_params)

_ = gc.collect()
```

### LightGBM

#### Stage 1
```{python}
#| code-summary: "Initial Tuning Trial for LightGBM models"

def lgb_objective_stage1(trial):
    params = {
        "device": "gpu",
        "gpu_platform_id": 1,   # My GPU is on platform 1
        "gpu_device_id": 0, 
        "objective": "binary",
        "metric": "auc",
        "boosting_type": "gbdt",
        "verbosity": -1,
        "tree_learner": "serial",
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "num_leaves": trial.suggest_int("num_leaves", 64, 512),
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.5, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.5, 1.0),
        "bagging_freq": trial.suggest_int("bagging_freq", 1, 10),
        "lambda_l1": trial.suggest_float("lambda_l1", 0, 5.0),
        "lambda_l2": trial.suggest_float("lambda_l2", 0, 5.0),
        "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
    }

    split_folder = 'train/lightgbm/stage1/data'
    nested_folder = 'train/lightgbm/stage1/data/training'
    
    # train (70) / valid (15) / test (15) split from 10% subsample
    subsample = file_subsample(f'higgs_data/train.parquet', split_folder, sample_fraction=0.1)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    
    # .15 / .85 = 0.1739
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)

        
    dtrain = lgb.Dataset(train_set, params={"label_column": "name:target", "header": True})
    dvalid = lgb.Dataset(valid_set, params={"label_column": "name:target", "header": True}, reference=train_set)

    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = lgb.train(
                params,
                dtrain,
                valid_sets=[dvalid],
                num_boost_round=100,
                callbacks=[
                    lgb.early_stopping(15),
                    LightGBMPruningCallback(trial, metric="auc", valid_name="valid_0")
                ],
            )

    # Calculate "test" AUC
    dtest = pd.read_csv(holdout_set, header=0)
    X_test = dtest.drop(columns=["target"])
    y_test = dtest["target"]

    preds = model.predict(X_test, num_iteration=model.best_iteration)
    
    return roc_auc_score(y_test, preds)
```

```{python}
#| echo: false

if os.path.exists("studies/lgb_stage1.pkl") and not FORCE_RERUN:
    with open("studies/lgb_stage1.pkl", "rb") as f:
        lgb_study1 = pickle.load(f)

else:
    lgb_study1 = optuna.create_study(
        study_name="lgb_stage1", 
        direction="maximize",
        sampler=TPESampler(n_startup_trials=20),
        pruner=MedianPruner(n_warmup_steps=20, interval_steps=5),
    )

    lgb_study1.optimize(lgb_objective_stage1, n_trials=100, gc_after_trial=True)

    with open("studies/lgb_stage1.pkl", "wb") as f:
        pickle.dump(lgb_study1, f)

print_study(lgb_study1)
```


#### Stage 2

```{python}
#| code-summary: "Second, Narrower Tuning Trial for LightGBM models"

def lgb_objective_stage2(trial):
    params = {
        "device": "gpu",
        "gpu_platform_id": 1,   # My GPU is on platform 1
        "gpu_device_id": 0, 
        "objective": "binary",
        "metric": "auc",
        "boosting_type": "gbdt",
        "verbosity": -1,
        "tree_learner": "serial",
        
        # Untouched as its related to the boosting rounds
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True), 
        
        # Hyperparameters to tune further
        "max_depth": new_range(trial, lgb_study1, "max_depth"),
        "num_leaves": new_range(trial, lgb_study1, "num_leaves"),
        "feature_fraction": new_range(trial, lgb_study1, "feature_fraction", upper_bound=1.0),
        "bagging_fraction": new_range(trial, lgb_study1, "bagging_fraction", upper_bound=1.0),
        "bagging_freq": new_range(trial, lgb_study1, "bagging_freq"),
        "lambda_l1": new_range(trial, lgb_study1, "lambda_l1"),
        "lambda_l2": new_range(trial, lgb_study1, "lambda_l2"),
        "min_child_samples": new_range(trial, lgb_study1, "min_child_samples"),
    }

    split_folder = 'train/lightgbm/stage2/data'
    nested_folder = 'train/lightgbm/stage2/data/training'
    
    # train (70) / valid (15) / test (15) split from 20% subsample
    subsample = file_subsample('higgs_data/train.parquet', split_folder, sample_fraction=0.2)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    
    # .15 / .85 = 0.1739
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)

        
    dtrain = lgb.Dataset(train_set, params={"label_column": "name:target", "header": True})
    dvalid = lgb.Dataset(valid_set, params={"label_column": "name:target", "header": True}, reference=train_set)

    # Absolutely no logging
    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = lgb.train(
                params,
                dtrain,
                valid_sets=[dvalid],
                num_boost_round=1000,
                callbacks=[
                    lgb.early_stopping(50),
                    LightGBMPruningCallback(trial, metric="auc", valid_name="valid_0")
                ],
            )

    # Calculate "test" AUC
    dtest = pd.read_csv(holdout_set, header=0)
    X_test = dtest.drop(columns=["target"])
    y_test = dtest["target"]

    preds = model.predict(X_test, num_iteration=model.best_iteration)
    
    return roc_auc_score(y_test, preds)
```

```{python}
#| echo: false

if os.path.exists("studies/lgb_stage2.pkl")  and not FORCE_RERUN:
    with open("studies/lgb_stage2.pkl", "rb") as f:
        lgb_study2 = pickle.load(f)

else:
    lgb_study2 = optuna.create_study(
        study_name="lgb_stage2", 
        direction="maximize",
        pruner = SuccessiveHalvingPruner(min_resource=100, reduction_factor=3),
    )

    lgb_study2.optimize(lgb_objective_stage2, n_trials=50, gc_after_trial=True)

    with open("studies/lgb_stage2.pkl", "wb") as f:
        pickle.dump(lgb_study2, f)

print_study(lgb_study2)
```

#### Stage 3
```{python}
#| code-summary: "Final fitting for LightGBM model"

def lgb_objective_stage3(best_params):
    params = {
        "device": "gpu",
        "gpu_platform_id": 1,   # My GPU is on platform 1
        "gpu_device_id": 0, 
        "objective": "binary",
        "metric": "auc",
        "boosting_type": "gbdt",
        "tree_learner": "serial",
    }

    split_folder = 'train/lightgbm/stage3/data'

    # train (80) / valid (20)  split from full training set
    train_set, valid_set = file_train_test_split('higgs_data/train.parquet', split_folder, test_size=0.20)
        
    dtrain = lgb.Dataset(train_set, params={"label_column": "name:target", "header": True})
    dvalid = lgb.Dataset(valid_set, params={"label_column": "name:target", "header": True}, reference=train_set)

    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            baseline_model = lgb.train(
                params,
                dtrain,
                valid_sets=[dvalid],
                num_boost_round=1000,
                callbacks=[
                    lgb.early_stopping(50),
                ],
            )

    baseline_model.save_model("models/baseline_lgb.txt", num_iteration=baseline_model.best_iteration)

    params.update(best_params)

    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            tuned_model = lgb.train(
                params,
                dtrain,
                valid_sets=[dvalid],
                num_boost_round=1000,
                callbacks=[
                    lgb.early_stopping(50),
                ],
            )

    tuned_model.save_model("models/tuned_lgb.txt", num_iteration=tuned_model.best_iteration)
```

```{python}
#| echo: false


best_params = {**lgb_study1.best_params, **lgb_study2.best_params}

conditions = [
    not os.path.exists("models/baseline_lgb.txt"), 
    not os.path.exists("models/tuned_lgb.txt"),
    FORCE_RERUN
]

if any(conditions):
    lgb_objective_stage3(best_params)

_ = gc.collect()
```


### CatBoost

#### Stage 1

```{python}
#| code-summary: "Initial Tuning Trial for CatBoost models"

def cat_objective_stage1(trial):
    params = {
        "task_type": "GPU",
        "metric_period": 5,
        "loss_function": "Logloss",
        "eval_metric": "AUC",
        "early_stopping_rounds": 15,
        "iterations": 100,
        "verbose": 0,
        "use_best_model": True,
        "depth": trial.suggest_int("depth", 3, 12),
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 0, 5.0),
        "bagging_temperature": trial.suggest_float("bagging_temperature", 0.0, 1.0),
        "random_strength": trial.suggest_float("random_strength", 0.0, 1.0),
        "border_count": trial.suggest_int("border_count", 32, 255),
    }

    split_folder = 'training/catboost/stage1/data'
    nested_folder = 'training/catboost/stage1/data/training'
    cd_file = './higgs_data/train.cd'

    subsample = file_subsample('higgs_data/train.parquet', split_folder, sample_fraction=0.1)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)
    
    dtrain = cat.Pool(train_set, column_description=cd_file, has_header=True, delimiter=',')
    dvalid = cat.Pool(valid_set, column_description=cd_file, has_header=True, delimiter=',')
    dtest = cat.Pool(holdout_set, column_description=cd_file, has_header=True, delimiter=',')

    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = cat.CatBoostClassifier(**params)
            model.fit(
                dtrain,
                eval_set=dvalid,
                column_description=cd_file,
            )

    # Evaluate on holdout set
    preds = model.predict_proba(dtest)[:, 1]
    y_test = pd.read_csv(holdout_set, usecols=["target"])
    
    return roc_auc_score(y_test, preds)
```

```{python}
#| echo: false

if os.path.exists("studies/cat_stage1.pkl") and not FORCE_RERUN:
    with open("studies/cat_stage1.pkl", "rb") as f:
        cat_study1 = pickle.load(f)

else:
    cat_study1 = optuna.create_study(
        study_name="cat_stage1", 
        direction="maximize",
        sampler=TPESampler(n_startup_trials=20),
        # no pruner for catboost
    )

    cat_study1.optimize(cat_objective_stage1, n_trials=100, gc_after_trial=True)

    with open("studies/cat_stage1.pkl", "wb") as f:
        pickle.dump(cat_study1, f)

print_study(cat_study1)
```

#### Stage 2

```{python}
#| code-summary: "Second, Narrower Tuning Trial for CatBoost models"

def cat_objective_stage2(trial):
    params = {
        "task_type": "GPU",
        "metric_period": 20,
        "loss_function": "Logloss",
        "eval_metric": "AUC",
        "early_stopping_rounds": 50,
        "iterations": 1000,
        "verbose": 0,
        "use_best_model": True,

        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        
        "depth": new_range(trial, cat_study1, "depth"),

        "l2_leaf_reg": new_range(trial, cat_study1, "l2_leaf_reg"),
        "bagging_temperature": new_range(trial, cat_study1, "bagging_temperature"),
        "random_strength": new_range(trial, cat_study1, "random_strength"),
        "border_count": new_range(trial, cat_study1, "border_count"),
    }

    split_folder = 'training/catboost/stage2/data'
    nested_folder = 'training/catboost/stage2/data/training'
    cd_file = './higgs_data/train.cd'

    subsample = file_subsample('higgs_data/train.parquet', split_folder, sample_fraction=0.2)
    train_valid_pool, holdout_set = file_train_test_split(subsample, split_folder, test_size=0.15)
    train_set, valid_set = file_train_test_split(train_valid_pool, nested_folder, test_size=0.1739)
    
    dtrain = cat.Pool(train_set, column_description=cd_file, has_header=True, delimiter=',')
    dvalid = cat.Pool(valid_set, column_description=cd_file, has_header=True, delimiter=',')
    dtest = cat.Pool(holdout_set, column_description=cd_file, has_header=True, delimiter=',')

    with open(os.devnull, 'w') as fnull:
        with redirect_stdout(fnull), redirect_stderr(fnull):
            model = cat.CatBoostClassifier(**params)
            model.fit(
                dtrain,
                eval_set=dvalid,
                column_description=cd_file,
            )

    # Evaluate on holdout set
    preds = model.predict_proba(dtest)[:, 1]
    y_test = pd.read_csv(holdout_set, usecols=["target"])
    
    return roc_auc_score(y_test, preds)
```

```{python}
#| echo: false

if os.path.exists("studies/cat_stage2.pkl")  and not FORCE_RERUN:
    with open("studies/cat_stage2.pkl", "rb") as f:
        cat_study2 = pickle.load(f)

else:
    cat_study2 = optuna.create_study(
        study_name="cat_stage2", 
        direction="maximize",
        # no pruner for catboost
    )

    cat_study2.optimize(cat_objective_stage2, n_trials=50, gc_after_trial=True)

    with open("studies/cat_stage2.pkl", "wb") as f:
        pickle.dump(cat_study2, f)

print_study(cat_study2)
```

#### Stage 3
```{python}
#| code-summary: "Final fitting for CatBoost model"

def cat_objective_stage3(best_params):
    params = {
        "task_type": "GPU",
        "metric_period": 20,
        "loss_function": "Logloss",
        "eval_metric": "AUC",
        "early_stopping_rounds": 50,
        "iterations": 1000,
        "verbose": 0,
        "use_best_model": True,
    }

    split_folder = 'training/catboost/stage3/data'
    cd_file = './higgs_data/train.cd'

    train_set, valid_set = file_train_test_split('higgs_data/train.parquet', split_folder, test_size=0.2)
    
    dtrain = cat.Pool(train_set, column_description=cd_file, has_header=True, delimiter=',')
    dvalid = cat.Pool(valid_set, column_description=cd_file, has_header=True, delimiter=',')

    baseline_model = cat.CatBoostClassifier(**params)
    baseline_model.fit(
        dtrain,
        eval_set=dvalid,
        column_description=cd_file,
    )

    baseline_model.save_model("models/baseline_cat.cbm", format="cbm")

    params.update(best_params)

    tuned_model = cat.CatBoostClassifier(**params)
    tuned_model.fit(
        dtrain,
        eval_set=dvalid,
        column_description=cd_file,
    )

    tuned_model.save_model("models/tuned_cat.cbm", format="cbm")


```


```{python}
#| echo: false


best_params = {**cat_study1.best_params, **cat_study2.best_params}

conditions = [
    not os.path.exists("models/baseline_cat.cbm"), 
    not os.path.exists("models/tuned_cat.cbm"),
    FORCE_RERUN
]

if any(conditions):
    cat_objective_stage3(best_params)

_ = gc.collect()
```

:::

## Results

The graph below shows that the performance difference between the baseline and tuned models is relatively modest. However, the XGBoost model displays noticeable signs of overfitting. Some variance between models was also observed during preliminary testing, likely due to the lack of cross-validation (a trade-off made for efficiency). Still, the results are encouraging. The CatBoost model, for instance, saw an increase in test AUC from 0.81 to 0.84. While Optuna shows clear potential for improving model performance, applying it within a more rigorous validation framework will be important moving forward.
```{python}
#| echo: false


if not os.path.exists('./higgs_data/aucs.csv'):
    from lazy_model_evaluation import evaluate_all_models

    datasets = ('./higgs_data/train.parquet', './higgs_data/test.parquet')
    evaluate_all_models('./models/', datasets)

df = pd.read_csv('./higgs_data/aucs.csv')

pivot = df.pivot(index='model', columns='data', values='auc').reset_index()

pivot['hyperparm_tuned'] = pivot['model'].str.split('_').str[0]
pivot['model_type'] = pivot['model'].str.split('_').str[1]
pivot.columns.name = None

```

```{python}
#| echo: false

fig, ax = plt.subplots(figsize=(10, 6))

palette = sns.color_palette("Paired")
# First: Train bars (background layer)
sns.barplot(
    x='model_type', y='train', hue='hyperparm_tuned', data=pivot,
    ax=ax, palette=palette[4:6], edgecolor='black', zorder=1
)

# Then: Test bars (foreground layer)
sns.barplot(
    x='model_type', y='test', hue='hyperparm_tuned', data=pivot,
    ax=ax, palette=palette[:2], edgecolor='black', zorder=2
)

# Put grid behind
ax.set_axisbelow(True)
ax.grid(True, which='major', axis='y', linestyle='--', zorder=0)

ax.set_xticks(range(3))
ax.set_xticklabels(['CatBoost', 'LightGBM', 'XGBoost'])
ax.set_xlabel('Model Type')
ax.set_ylabel('AUC')
ax.set_title('Test AUC and Train-Test Difference: Baseline vs. Tuned Models')


handles, labels = ax.get_legend_handles_labels()
custom_labels = ['Train - Baseline', 'Train - Tuned', 'Test - Baseline', 'Test - Tuned']
ax.legend(handles, custom_labels, title="Model Instance")
sns.move_legend(ax, "right", bbox_to_anchor=(1.25, 0.85), frameon=False)

plt.tight_layout()
plt.show()


# Show raw numbers as table
display(HTML(pivot[['model_type', 'hyperparm_tuned', 'train', 'test']].to_html()))
```

In conclusion, I'd say optuna is a wonderful tool to find a great hyperparameter configuration for your models. It's easy to use and can be integrated into your workflow with just a few lines of code. Highly recommend it to anyone looking to improve their model performance.
